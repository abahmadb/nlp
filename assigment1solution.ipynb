{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model 0:"
      ],
      "metadata": {
        "id": "eA4pbT2UQtzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import all required libraries\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torch import nn\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from collections import defaultdict, Counter, OrderedDict\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# plotting cosmetics\n",
        "%config InlineBackend.figure_format = 'svg' \n",
        "#%config InlineBackend.figure_format = 'retina' \n",
        "plt.style.use('seaborn')"
      ],
      "metadata": {
        "id": "AYfN51NhQ0rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(infile):\n",
        "    stats = defaultdict(Counter)\n",
        "    with open(infile) as f:\n",
        "        for line in f:\n",
        "            #sense, lemma, _, = l.split(maxsplit=2)\n",
        "            sense, lemma, position, text= line.split(maxsplit=4)\n",
        "            stats[lemma][sense][position][text] += 1\n",
        "\n",
        "    return { line: stats_line.most_common(1)[0][0] for line, stats_line in stats.items() }\n",
        "    def train(infile):\n",
        "      stats = defaultdict(Counter)\n",
        "    with open(infile) as f:\n",
        "        for l in f:\n",
        "            sense, lemma, _, = l.split(maxsplit=2)\n",
        "            stats[lemma][sense] += 1\n",
        "\n",
        "    return { l: stats_l.most_common(1)[0][0] for l, stats_l in stats.items() }\n",
        "\n",
        "\n",
        "\n",
        "def train(infile):\n",
        "    stats = defaultdict(Counter)\n",
        "    with open(infile) as f:\n",
        "        for l in f:\n",
        "            sense, lemma, _, = l.split(maxsplit=2)\n",
        "            stats[lemma][sense] += 1\n",
        "\n",
        "    return { l: stats_l.most_common(1)[0][0] for l, stats_l in stats.items() }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run(model, infile, outfile):\n",
        "    f_out = open(outfile, 'w')\n",
        "    with open(infile) as f_in:\n",
        "        for l in f_in:\n",
        "            sense, lemma, _, = l.split(maxsplit=2)\n",
        "            print(model[lemma], file=f_out)\n",
        "    f_out.close()\n",
        "\n",
        "TRAIN_DATA = 'wsd_train.txt'\n",
        "TEST_DATA = 'wsd_test_blind.txt'\n",
        "OUTPUT = 'dummy_baseline.txt'\n",
        "\n",
        "model = train(TRAIN_DATA)\n",
        "run(model, TEST_DATA, OUTPUT)"
      ],
      "metadata": {
        "id": "ZStbfsNKQ1DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 evaluate.py wsd_test.txt dummy_baseline.txt"
      ],
      "metadata": {
        "id": "exEn7CrORP8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1: Continuous Bag-of-Words approach\n",
        "Representing documents for neural networks: "
      ],
      "metadata": {
        "id": "ywj-6RK6TEhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBoWDocumentRepresentation(nn.Module):\n",
        "    \n",
        "    def __init__(self, voc_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "       \n",
        "        embedded = self.embedding(X)\n",
        "        cbow_repr = embedded.mean(dim=1)\n",
        "        return cbow_repr\n",
        "\n",
        "\n",
        "\n",
        "def read_data(corpus_file):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            sense, lemma, position, text= line.split(\"\\t\")\n",
        "            X.append(text)\n",
        "            Y.append(sense)\n",
        "    return X, Y\n",
        "    \n",
        "\n",
        "\n",
        "# Encoding documents for neural networks\n",
        "\n",
        "PAD = '___PAD___'\n",
        "UNKNOWN = '___UNKNOWN___'\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Manages the numerical encoding of the vocabulary.\"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer=None, max_voc_size=None):\n",
        "\n",
        "        # String-to-integer mapping\n",
        "        self.stoi = None\n",
        "\n",
        "        # Integer-to-string mapping\n",
        "        self.itos = None\n",
        "\n",
        "        # Tokenizer that will be used to split document strings into words.\n",
        "        if tokenizer:\n",
        "            self.tokenizer = tokenizer\n",
        "        else:\n",
        "            self.tokenizer = lambda s: s.split()\n",
        "\n",
        "        # Maximally allowed vocabulary size.\n",
        "        self.max_voc_size = max_voc_size\n",
        "        \n",
        "    def build(self, docs):\n",
        "        \"\"\"Builds the vocabulary, based on a set of documents.\"\"\"\n",
        "        \n",
        "        # Sort all words by frequency\n",
        "        word_freqs = Counter(w for doc in docs for w in self.tokenizer(doc))\n",
        "        word_freqs = sorted(((f, w) for w, f in word_freqs.items()), reverse=True)\n",
        "\n",
        "        # Build the integer-to-string mapping. The vocabulary starts with the two dummy symbols,\n",
        "        # and then all words, sorted by frequency. Optionally, limit the vocabulary size.\n",
        "        if self.max_voc_size:\n",
        "            self.itos = [PAD, UNKNOWN] + [ w for _, w in word_freqs[:self.max_voc_size-2] ]\n",
        "        else:\n",
        "            self.itos = [PAD, UNKNOWN] + [ w for _, w in word_freqs ]\n",
        "\n",
        "        # Build the string-to-integer map by just inverting the aforementioned map.\n",
        "        self.stoi = { w: i for i, w in enumerate(self.itos) }\n",
        "        \n",
        "    def encode(self, docs):\n",
        "        \"\"\"Encodes a set of documents.\"\"\"\n",
        "        unkn_index = self.stoi[UNKNOWN]\n",
        "        return [[self.stoi.get(w, unkn_index) for w in self.tokenizer(doc)] for doc in docs]\n",
        "\n",
        "    def get_unknown_idx(self):\n",
        "        \"\"\"Returns the integer index of the special dummy word representing unknown words.\"\"\"\n",
        "        return self.stoi[UNKNOWN]\n",
        "    \n",
        "    def get_pad_idx(self):\n",
        "        \"\"\"Returns the integer index of the special padding dummy word.\"\"\"\n",
        "        return self.stoi[PAD]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "  \n",
        "\n",
        "\n",
        "# Managing batches\n",
        "class DocumentDataset(Dataset):\n",
        "    \"\"\"A Dataset that stores a list of documents and their corresponding category labels.\"\"\"\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "class DocumentBatcher:\n",
        "    \"\"\"A collator that builds a batch from a number of documents.\"\"\"\n",
        "    \n",
        "    def __init__(self, voc):\n",
        "        # Find the integer index of the dummy padding word.\n",
        "        self.pad = voc.get_pad_idx()\n",
        "    \n",
        "    def __call__(self, XY):\n",
        "        \"\"\"Build a batch from a number of documents. Returns two tensors X and Y, where\n",
        "        X is the document tensor, of shape [n_docs, max_doc_length]\n",
        "\n",
        "        and \n",
        "        \n",
        "        Y is the label tensor, of shape [n_docs].\n",
        "        \"\"\"\n",
        "        \n",
        "        # How long is the longest document in this batch?\n",
        "        max_len = max(len(x) for x, _ in XY)\n",
        "\n",
        "        # Build the document tensor. We pad the shorter documents so that all documents\n",
        "        # have the same length.\n",
        "        Xpadded = torch.as_tensor([x + [self.pad]*(max_len-len(x)) for x, _ in XY])\n",
        "\n",
        "        # Build the label tensor.\n",
        "        Y = torch.as_tensor([y for _, y in XY])\n",
        "\n",
        "        return Xpadded, Y\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# Implementing the document classifier infrastructure\n",
        "class TextClassifier:\n",
        "    \"\"\"A text classifier based on a neural network.\"\"\"\n",
        "    \n",
        "    def __init__(self, params, model_factory):\n",
        "        self.params = params\n",
        "        self.model_factory = model_factory\n",
        "        \n",
        "    def epoch(self, batches, optimizer=None):\n",
        "        \"\"\"Runs the neural network for one epoch, using the given batches.\n",
        "        If an optimizer is provided, this is training data and we will update the model\n",
        "        after each batch. Otherwise, this is assumed to be validation data.\n",
        "        \n",
        "        Returns the loss and accuracy over the epoch.\"\"\"\n",
        "        n_correct = 0\n",
        "        n_instances = 0\n",
        "        total_loss = 0\n",
        "        \n",
        "        for Xbatch, Ybatch in batches:\n",
        "            \n",
        "            # If we're using the GPU, move the batch there.\n",
        "            Xbatch = Xbatch.to(self.params.device)\n",
        "            Ybatch = Ybatch.to(self.params.device)\n",
        "\n",
        "\n",
        "            # Compute the predictions for this batch.\n",
        "            scores = self.model(Xbatch)\n",
        "\n",
        "            # Compute the loss for this batch.\n",
        "            loss = self.loss(scores, Ybatch)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            n_instances += Ybatch.shape[0]\n",
        "\n",
        "            # Compute the number of correct predictions, for the accuracy.\n",
        "            guesses = scores.argmax(dim=1)\n",
        "            n_correct += (guesses == Ybatch).sum().item()\n",
        "\n",
        "            # If this is training data, update the model.\n",
        "            if optimizer:\n",
        "                optimizer.zero_grad()                \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "           \n",
        "        return total_loss/len(batches), n_correct/n_instances\n",
        "    \n",
        "    def preprocess(self, X, Y):\n",
        "        \"\"\"Carry out the document preprocessing, then build `DataLoader`s for the training and validation sets.\"\"\"\n",
        "        Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "        \n",
        "        self.voc = Vocabulary(max_voc_size=self.params.max_voc_size)\n",
        "        self.voc.build(Xtrain)\n",
        "        self.lbl_enc = LabelEncoder()\n",
        "        self.lbl_enc.fit(Ytrain)\n",
        "\n",
        "        self.voc_size = len(self.voc)\n",
        "        self.n_classes = len(self.lbl_enc.classes_)\n",
        "        \n",
        "        batcher = DocumentBatcher(self.voc)\n",
        "        \n",
        "        train_dataset = DocumentDataset(self.voc.encode(Xtrain), self.lbl_enc.transform(Ytrain))\n",
        "        self.train_loader = DataLoader(train_dataset, self.params.batch_size, shuffle=True,\n",
        "                                  collate_fn=batcher)\n",
        "        val_dataset = DocumentDataset(self.voc.encode(Xval), self.lbl_enc.transform(Yval))\n",
        "        self.val_loader = DataLoader(val_dataset, self.params.batch_size, shuffle=True,\n",
        "                                collate_fn=batcher)\n",
        "    \n",
        "    \n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"Train the model. We assume that a dataset and a model have already been provided.\"\"\"\n",
        "        par = self.params\n",
        "\n",
        "        self.preprocess(X, Y)\n",
        "\n",
        "        # Call the factory to create the model.\n",
        "        self.model = self.model_factory(self, par)\n",
        "\n",
        "        # If we're using a GPU, put the model there.\n",
        "        self.model.to(par.device)\n",
        "    \n",
        "        # Declare a loss function, in this case the cross-entropy.\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # An optimizer for updating the neural network. We use the Adam optimizer.\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=par.eta, weight_decay=par.decay)\n",
        "\n",
        "        # We'll log the loss and accuracy scores encountered during training.\n",
        "        self.history = defaultdict(list)\n",
        "        \n",
        "        for epoch in range(1, par.n_epochs+1):\n",
        "\n",
        "            t0 = time.time()\n",
        "            \n",
        "            # Set the model in training mode, enabling dropout modules.\n",
        "            self.model.train()\n",
        "            \n",
        "            # Run the model on the training data.\n",
        "            train_loss, train_acc = self.epoch(self.train_loader, optimizer)\n",
        "            \n",
        "            # Set the model in evaluation mode, disabling dropout modules.\n",
        "            self.model.eval()\n",
        "\n",
        "            # Run the model on the validation data.            \n",
        "            val_loss, val_acc = self.epoch(self.val_loader)\n",
        "            \n",
        "            t1 = time.time()\n",
        "\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "            self.history['time'].append(t1-t0)\n",
        "            \n",
        "            if epoch % 5 == 0:\n",
        "                print(f'Epoch {epoch}: train loss:{train_loss:.4f}, train acc: {train_acc:.4f}, '\n",
        "                      + f'val loss: {val_loss:.4f}, val acc: {val_acc:.4f}, time: {t1-t0:.4f}')        \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"Run a trained document classifier on a set of documents and return the predictions.\"\"\"\n",
        "        batcher = DocumentBatcher(self.voc)\n",
        "        \n",
        "        # Build a DataLoader to generate the batches, as above.\n",
        "        dummy_labels = [self.lbl_enc.classes_[0] for x in X]        \n",
        "        dataset = DocumentDataset(self.voc.encode(X), self.lbl_enc.transform(dummy_labels))\n",
        "        loader = DataLoader(dataset, self.params.batch_size, collate_fn=batcher)\n",
        "\n",
        "        # Apply the model to all the batches and aggregate the predictions.\n",
        "        self.model.eval()\n",
        "        output = []\n",
        "        for Xbatch, Ybatch in loader:\n",
        "            Xbatch = Xbatch.to(self.params.device)\n",
        "            Ybatch = Ybatch.to(self.params.device)\n",
        "            scores = self.model(Xbatch)\n",
        "            guesses = scores.argmax(dim=1)\n",
        "            output.extend(self.lbl_enc.inverse_transform(guesses.cpu().numpy()))\n",
        "        return output\n",
        "\n",
        "\n",
        "# Training the model\n",
        "class TextClassifierParameters:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    max_voc_size = None    \n",
        "    n_epochs = 50\n",
        "    batch_size = 64    \n",
        "    eta = 3e-3\n",
        "    decay = 1e-6\n",
        "    emb_dim = 64\n",
        "    dropout = 0.5\n",
        "    \n",
        "\n",
        "def factory(clf, params):\n",
        "    return nn.Sequential(\n",
        "        CBoWDocumentRepresentation(clf.voc_size, params.emb_dim),\n",
        "        nn.Dropout(params.dropout),\n",
        "        nn.Linear(in_features=params.emb_dim, out_features=clf.n_classes)\n",
        "    )\n",
        "\n",
        "\n",
        "def main_cbow():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    X, Y = read_data('wsd_train.txt')   \n",
        "    params = TextClassifierParameters()\n",
        "    \n",
        "    clf = TextClassifier(params, factory)\n",
        "        \n",
        "    clf.fit(X, Y)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(clf.history['train_loss'])\n",
        "    plt.plot(clf.history['val_loss'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(clf.history['train_acc'])\n",
        "    plt.plot(clf.history['val_acc'])\n",
        "    \n",
        "    return clf\n",
        "\n",
        "clf = main_cbow()    "
      ],
      "metadata": {
        "id": "u39j9_HmRjts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(['i watch movie', 'I have spare hour to spend'])"
      ],
      "metadata": {
        "id": "ahKxytOuYAut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbors(emb, voc, word, n_neighbors=5):\n",
        "        \n",
        "    word_index = torch.as_tensor([voc.stoi[word]])\n",
        "    word_index = word_index.to(emb.weight.device)\n",
        "    test_emb = emb(word_index)\n",
        "    sim_func = nn.CosineSimilarity(dim=1)\n",
        "    cosine_scores = sim_func(test_emb, emb.weight)\n",
        "    near_nbr = cosine_scores.topk(n_neighbors+1)\n",
        "    topk_cos = near_nbr.values[1:]\n",
        "    topk_indices = near_nbr.indices[1:]\n",
        "    out = [ (voc.itos[ix.item()], cos.item()) for ix, cos in zip(topk_indices, topk_cos) ]\n",
        "    return out"
      ],
      "metadata": {
        "id": "xlkbhYzCYOa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.model"
      ],
      "metadata": {
        "id": "irQVhik1YhEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbors(clf.model[0].embedding, clf.voc, 'author', n_neighbors=10)"
      ],
      "metadata": {
        "id": "Y_mAcqkxYjoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_magnitude(emb, voc, n_max=5):\n",
        "\n",
        "    # emb.weight has shape (voc_size, emb_dim)\n",
        "\n",
        "    sq_all = (emb.weight**2).sum(dim=1)\n",
        "    \n",
        "    # sq_all has shape voc_size\n",
        "\n",
        "    topk_sq, topk_indices = sq_all.topk(n_max)\n",
        "        \n",
        "    out = [ (voc.itos[ix.item()], sq.item()) for ix, sq in zip(topk_indices, topk_sq) ]\n",
        "    return out"
      ],
      "metadata": {
        "id": "NsGaiqgTYyvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_magnitude(clf.model[0].embedding, clf.voc, n_max=25)"
      ],
      "metadata": {
        "id": "2GxEuxS3Y1e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embeddings_pca(emb, voc, words):\n",
        "    w_ix = [voc.stoi[word] for word in words]\n",
        "\n",
        "    example_vectors = emb.weight[w_ix].detach().cpu().numpy()\n",
        "    example_vectors -= example_vectors.mean(axis=0)\n",
        "    twodim = TruncatedSVD(n_components=2).fit_transform(example_vectors)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r', s=5)\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.02, y, word)\n",
        "    plt.axis('off')\n",
        "\n",
        "words = ['positive', 'worst', 'serve', 'holds', 'regular','sees', 'followed', 'job', 'driving','see', 'follow', 'extend','build', 'bad', 'kept','sees']\n",
        "         \n",
        "plot_embeddings_pca(clf.model[0].embedding, clf.voc, words)"
      ],
      "metadata": {
        "id": "Zmy09zaEY_IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Model 2: Unidirectional LSTM"
      ],
      "metadata": {
        "id": "9U9NCS40zV0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_embeddings, num_classes, embedding_dim, hidden_size):\n",
        "        super().__init__()  \n",
        "\n",
        "        vocab_size = len(num_embeddings.vocab)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5) # Dropout layer\n",
        "        \n",
        "        self.a = nn.Linear( hidden_size,hidden_size)\n",
        "        num_class = len(num_classes.vocab) \n",
        "        self.y = nn.Linear(hidden_size, num_class)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        output, (h_n, c_n) = self.lstm(x)\n",
        "        return self.softmax(self.y(h_n[-1]))\n",
        "\n",
        "\n",
        "def read_data(corpus_file, doc_start, with_padding = True):\n",
        "\n",
        "    text = torchtext.legacy.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "    label = torchtext.legacy.data.LabelField(is_target=True)\n",
        "    datafields = [('text', text), ('label', label)]\n",
        "    column_labeling=0\n",
        "    if with_padding == True:\n",
        "        pad_string = '<pad>'\n",
        "        sentence_length = 160\n",
        "        half_sentence_length = int(sentence_length/2)\n",
        "        with open(corpus_file, encoding='utf-8') as f:\n",
        "            examples = []\n",
        "            for line in f:\n",
        "                columns = line.strip().split(maxsplit=doc_start)\n",
        "                position_of_wordtype = int(columns[2])\n",
        "                doc = columns[-1]\n",
        "                doc_string_vector = doc.split()\n",
        "                temp_pad = [pad_string for x in range(0,80)]\n",
        "\n",
        "                swap_padding = []\n",
        "                swap_padding.extend(temp_pad)\n",
        "                swap_padding.extend(doc_string_vector)\n",
        "                swap_padding.extend(temp_pad)\n",
        "\n",
        "                sliced_doc = swap_padding[position_of_wordtype:position_of_wordtype + 160]\n",
        "\n",
        "                if len(sliced_doc) != 160:\n",
        "                    print(sliced_doc)\n",
        "                    raise RuntimeError\n",
        "\n",
        "                sliced_doc = \" \".join(sliced_doc)\n",
        "                label = columns[column_labeling]\n",
        "\n",
        "                examples.append(torchtext.legacy.data.Example.fromlist([sliced_doc, label], datafields))\n",
        "    else:\n",
        "        with open(corpus_file, encoding='utf-8') as f:\n",
        "            examples = []\n",
        "            for line in f:\n",
        "                columns = line.strip().split(maxsplit=doc_start)\n",
        "                doc = columns[-1]\n",
        "                label = columns[column_labeling]\n",
        "                examples.append(torchtext.data.Example.fromlist([doc, label], datafields))\n",
        "    raw_data = torchtext.legacy.data.Dataset(examples, datafields,filter_pred=None)\n",
        "\n",
        "    # Read complete dataset to get set of word-types. E.i 'keep', 'line'...\n",
        "    filter_function = None\n",
        "    lemmas = set()\n",
        "    for example in raw_data.examples:\n",
        "        lemmas.add(example.label.split(\"%\", 1)[0])\n",
        "    lemmas = list(lemmas)\n",
        "\n",
        "    # Create cleaned datasets for each word-type\n",
        "    cleaned_datasets = OrderedDict()\n",
        "    for a_lemma in lemmas:\n",
        "        filter_function = lambda ex: ex.label.split(\"%\", 1)[0] == a_lemma\n",
        "        text = torchtext.legacy.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "        label = torchtext.legacy.data.LabelField(is_target=True)\n",
        "        datafields = [('text', text), ('label', label)]\n",
        "\n",
        "        cleaned_data_set = torchtext.legacy.data.Dataset(examples, datafields, filter_pred=filter_function)\n",
        "        cleaned_datasets[a_lemma] = (cleaned_data_set, text, label)\n",
        "    return cleaned_datasets\n",
        "\n",
        "\n",
        "def evaluate_validation(scores, loss_function, gold):\n",
        "    guesses = scores.argmax(dim=1)\n",
        "    n_correct = (guesses == gold).sum().item()\n",
        "    return n_correct, loss_function(scores, gold).item()\n",
        "\n",
        "\n",
        "use_pretrained = True\n",
        "cleaned_datasets = read_data('wsd_train.txt', doc_start=4)\n",
        "\n",
        "models = OrderedDict()\n",
        "max_verifications = OrderedDict()\n",
        "model_vocabs = OrderedDict()\n",
        "model_label_vocabs = OrderedDict()\n",
        "\n",
        "for lemma, cleaned_dataset in cleaned_datasets.items():\n",
        "    dataset = cleaned_dataset[0]\n",
        "    text = cleaned_dataset[1]\n",
        "    label = cleaned_dataset[2]\n",
        "\n",
        "    train, valid = dataset.split([0.7, 0.3])\n",
        "\n",
        "    if use_pretrained:\n",
        "        text.build_vocab(train, vectors=\"glove.6B.100d\")\n",
        "    else:\n",
        "        text.build_vocab(train, max_size=10000)\n",
        "    \n",
        "    model_vocabs[lemma] = text.vocab\n",
        "    label.build_vocab(train)\n",
        "    model_label_vocabs[lemma] = label.vocab\n",
        "        \n",
        "    model =LSTMClassifier(text, label, embedding_dim=100, hidden_size=100)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    training_executon = torchtext.legacy.data.Iterator(train, device=device, batch_size=128, repeat=False,train=True,\n",
        "        sort=False)\n",
        "\n",
        "    validation_execution = torchtext.legacy.data.Iterator(valid, device=device, batch_size=128, repeat=False,train=False, sort=False)\n",
        "\n",
        "    loss_function = torch.nn.CrossEntropyLoss()   \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=2, momentum=0.1) \n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.96)\n",
        "\n",
        "\n",
        "    train_batches = list(training_executon)\n",
        "    valid_batches = list(validation_execution)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    max_val_acc = -1\n",
        "\n",
        "    print(f\"Training lemma for: {lemma}\")\n",
        "    for i in range(50):\n",
        "        t0 = time.time()\n",
        "        loss_sum = 0\n",
        "        n_batches = 0\n",
        "        model.train()\n",
        "        \n",
        "        for batch in train_batches:         \n",
        "            scores = model(batch.text)\n",
        "            loss = loss_function(scores, batch.label)\n",
        "            optimizer.zero_grad()            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "            n_batches += 1\n",
        "        \n",
        "        train_loss = loss_sum / n_batches\n",
        "        history['train_loss'].append(train_loss)\n",
        "        \n",
        "        n_correct = 0\n",
        "        n_valid = len(valid)\n",
        "        loss_sum = 0\n",
        "        n_batches = 0\n",
        "\n",
        "        model.eval()\n",
        "        \n",
        "        for batch in valid_batches:\n",
        "            scores = model(batch.text)\n",
        "            n_corr_batch, loss_batch = evaluate_validation(scores, loss_function, batch.label)\n",
        "            loss_sum += loss_batch\n",
        "            n_correct += n_corr_batch\n",
        "            n_batches += 1\n",
        "        val_acc = n_correct / n_valid\n",
        "        val_loss = loss_sum / n_batches\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if val_acc > max_val_acc:\n",
        "            max_val_acc = val_acc\n",
        "            models[lemma] = copy.deepcopy(model)\n",
        "            max_verifications[lemma] = max_val_acc\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        t1 = time.time()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch {i+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}, val acc: {val_acc:.4f}, time = {t1-t0:.4f}, lr = {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "    plt.plot(history['train_loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.plot(history['val_acc'])\n",
        "    plt.legend(['training loss', 'validation loss', 'validation accuracy'])\n",
        "    plt.show()\n",
        "    "
      ],
      "metadata": {
        "id": "gMQUF7U-xipc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Unidirectional LSTM"
      ],
      "metadata": {
        "id": "0_nMx_4Fgfbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_test_data(corpus_file, doc_start):\n",
        "    TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "    WORDTYPE = torchtext.legacy.data.Field()\n",
        "    datafields = [('text', TEXT), ('wordtype', WORDTYPE)]\n",
        "    pad_string = '<pad>'\n",
        "    sentence_length = 160\n",
        "    half_sentence_length = int(sentence_length/2)\n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        examples = []\n",
        "        for line in f:\n",
        "            columns = line.strip().split(maxsplit=doc_start)\n",
        "            position_of_wordtype = int(columns[2])\n",
        "\n",
        "            # Split the long string doc into array and extract words before the wordtype.\n",
        "            doc = columns[-1]\n",
        "            doc_string_vector = doc.split()\n",
        "            temp_pad = [pad_string for x in range(0,80)]\n",
        "\n",
        "            swap_padding = []\n",
        "            swap_padding.extend(temp_pad)\n",
        "            swap_padding.extend(doc_string_vector)\n",
        "            swap_padding.extend(temp_pad)\n",
        "\n",
        "            sliced_doc = swap_padding[position_of_wordtype:position_of_wordtype + 160]\n",
        "\n",
        "            if len(sliced_doc) != 160:\n",
        "                print(sliced_doc)\n",
        "                raise RuntimeError\n",
        "\n",
        "            sliced_doc = \" \".join(sliced_doc)\n",
        "\n",
        "            wordtype = columns[1].split('.')[0]\n",
        "            examples.append(torchtext.legacy.data.Example.fromlist([sliced_doc, wordtype], datafields))\n",
        "    dataset = torchtext.legacy.data.Dataset(examples, datafields)\n",
        "    return (dataset, TEXT, WORDTYPE)"
      ],
      "metadata": {
        "id": "lki9IzByxivH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = read_test_data('wsd_test_blind.txt', doc_start=4)\n",
        "for example in test_dataset[0].examples:\n",
        "    test_example_string = example.text\n",
        "    example_wordtype = example.wordtype[0]\n",
        "\n",
        "    # Load vocabs\n",
        "    example_vocab = model_vocabs[example_wordtype]\n",
        "    example_label_vocabs = model_label_vocabs[example_wordtype]\n",
        "\n",
        "\n",
        "    # Encode string\n",
        "    encoded_example = torch.tensor([example_vocab.stoi[x] for x in test_example_string],device=device, requires_grad = False)\n",
        "\n",
        "    # Load correct model\n",
        "    model = models[example_wordtype]\n",
        "    model.lstm.flatten_parameters()\n",
        "\n",
        "    # Get prediction\n",
        "    scores = model(encoded_example.unsqueeze(1))\n",
        "    prediction = scores.argmax(dim=1)\n",
        "    \n",
        "    # Decode prediction\n",
        "    decoded_prediction = example_label_vocabs.itos[prediction]\n",
        "    print(decoded_prediction)"
      ],
      "metadata": {
        "id": "WiGxBPYbxix9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3: Bidirectional LSTM"
      ],
      "metadata": {
        "id": "ZB-30leVzNUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BILSTMModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_embeddings, num_classes, embedding_dim, hidden_dim, update_pretrained=False):\n",
        "        super().__init__()        \n",
        "\n",
        "        \n",
        "        vocab_size = len(num_embeddings.vocab)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True )\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.5) # Dropout layer\n",
        "  \n",
        "        self.a = nn.Linear(2 * hidden_dim, 2 * hidden_dim)  # hidden layer\n",
        "        # classification layers\n",
        "        num_class = len(num_classes.vocab) \n",
        "        self.y = nn.Linear(2 * hidden_dim, num_class)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.lstm(x)\n",
        "\n",
        "        self.batch_size = output.size()[1]\n",
        "        self.num_directions = 2\n",
        "        output = output.view(-1, self.batch_size,self.num_directions,  self.lstm.hidden_size)\n",
        "        forward = output[80 - 1, :, 0, :]\n",
        "        backward = output[80, :, 1, :]\n",
        "        output = torch.cat([forward, backward], dim=1)\n",
        "        dropped_output = self.dropout(output)\n",
        "        hidden = self.a(dropped_output)\n",
        "        dropped_hidden = self.dropout(hidden)\n",
        "        return self.softmax(self.y(dropped_hidden))\n",
        "\n",
        "### This read data function will read the dataset and after reading the dataset, we did data pre-processing.\n",
        "### We did paading from both right and left side.\n",
        "def read_data(corpus_file, doc_start, with_padding = True):\n",
        "\n",
        "    text = torchtext.legacy.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "    label = torchtext.legacy.data.LabelField(is_target=True)\n",
        "    datafields = [('text', text), ('label', label)]\n",
        "    column_labeling=0\n",
        "    if with_padding == True:\n",
        "        pad_string = '<pad>'\n",
        "        sentence_length = 160\n",
        "        half_sentence_length = int(sentence_length/2)\n",
        "        with open(corpus_file, encoding='utf-8') as f:\n",
        "            examples = []\n",
        "            for line in f:\n",
        "                columns = line.strip().split(maxsplit=doc_start)\n",
        "                position_of_wordtype = int(columns[2])\n",
        "\n",
        "                doc = columns[-1]\n",
        "                doc_string_vector = doc.split()\n",
        "                temp_pad = [pad_string for x in range(0,80)]\n",
        "\n",
        "                swap_padding = []\n",
        "                swap_padding.extend(temp_pad)\n",
        "                swap_padding.extend(doc_string_vector)\n",
        "                swap_padding.extend(temp_pad)\n",
        "\n",
        "                sliced_doc = swap_padding[position_of_wordtype:position_of_wordtype + 160]\n",
        "\n",
        "                if len(sliced_doc) != 160:\n",
        "                    print(sliced_doc)\n",
        "                    raise RuntimeError\n",
        "\n",
        "                sliced_doc = \" \".join(sliced_doc)\n",
        "                label = columns[column_labeling]\n",
        "\n",
        "                examples.append(torchtext.legacy.data.Example.fromlist([sliced_doc, label], datafields))\n",
        "    else:\n",
        "        with open(corpus_file, encoding='utf-8') as f:\n",
        "            examples = []\n",
        "            for line in f:\n",
        "                columns = line.strip().split(maxsplit=doc_start)\n",
        "                doc = columns[-1]\n",
        "                label = columns[column_labeling]\n",
        "                examples.append(torchtext.data.Example.fromlist([doc, label], datafields))\n",
        "    raw_data = torchtext.legacy.data.Dataset(examples, datafields,filter_pred=None)\n",
        "\n",
        "    # Read complete dataset to get set of word-types. E.i 'keep', 'line'...\n",
        "    filter_function = None\n",
        "    lemmas = set()\n",
        "    for example in raw_data.examples:\n",
        "        lemmas.add(example.label.split(\"%\", 1)[0])\n",
        "    lemmas = list(lemmas)\n",
        "\n",
        "    # Create clean datasets for each word-type\n",
        "    cleaned_datasets = OrderedDict()\n",
        "    for a_lemma in lemmas:\n",
        "        filter_function = lambda ex: ex.label.split(\"%\", 1)[0] == a_lemma\n",
        "        text = torchtext.legacy.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "        label = torchtext.legacy.data.LabelField(is_target=True)\n",
        "        datafields = [('text', text), ('label', label)]\n",
        "\n",
        "        cleaned_data_set = torchtext.legacy.data.Dataset(examples, datafields, filter_pred=filter_function)\n",
        "        cleaned_datasets[a_lemma] = (cleaned_data_set, text, label)\n",
        "    return cleaned_datasets\n",
        "\n",
        "\n",
        "def evaluate_validation(scores, loss_function, gold):\n",
        "    guesses = scores.argmax(dim=1)\n",
        "    n_correct = (guesses == gold).sum().item()\n",
        "    return n_correct, loss_function(scores, gold).item()\n",
        "\n",
        "\n",
        "use_pretrained = True\n",
        "cleaned_datasets = read_data('wsd_train.txt', doc_start=4)\n",
        "\n",
        "# Vectorizing the data\n",
        "models = OrderedDict()\n",
        "max_verifications = OrderedDict()\n",
        "model_vocabs = OrderedDict()\n",
        "model_label_vocabs = OrderedDict()\n",
        "\n",
        "for lemma, cleaned_dataset in cleaned_datasets.items():\n",
        "    dataset = cleaned_dataset[0]\n",
        "    text = cleaned_dataset[1]\n",
        "    label = cleaned_dataset[2]\n",
        "\n",
        "    train, valid = dataset.split([0.7, 0.3])\n",
        "\n",
        "    if use_pretrained:\n",
        "      text.build_vocab(train, vectors=\"glove.6B.100d\")\n",
        "    else:        \n",
        "      text.build_vocab(train, max_size=10000)\n",
        "    \n",
        "    model_vocabs[lemma] = text.vocab\n",
        "    label.build_vocab(train)\n",
        "    model_label_vocabs[lemma] = label.vocab\n",
        "        \n",
        "    model = BILSTMModel(text, label, embedding_dim=100, hidden_dim=100, update_pretrained=True)\n",
        "\n",
        "# Before we start, let's check whether CUDA (the type of GPU needed for neural network training) is available in our system.\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "# Now, after all this preparatory work, let's train the document classifier. \n",
        "# We read the data, preprocess the data, declare a BiLSTM, and train the classifier.\n",
        "# Finally, we plot the losses and accuracies we have seen during training\n",
        "\n",
        "    training_execution = torchtext.legacy.data.Iterator(train, device=device,batch_size=128, repeat=False,train=True,sort=False)\n",
        "    validation_execution = torchtext.legacy.data.Iterator(valid,device=device, batch_size=128,repeat=False, train=False,sort=False)\n",
        "\n",
        "    loss_function = torch.nn.CrossEntropyLoss()   \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=2, momentum=0.1) \n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.96)\n",
        "\n",
        "\n",
        "    train_batches = list(training_execution)\n",
        "    valid_batches = list(validation_execution)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    max_val_acc = -1\n",
        "\n",
        "    print(f\"Training lemma for:  {lemma}\")\n",
        "    for i in range(50):\n",
        "        t0 = time.time()\n",
        "        loss_sum = 0\n",
        "        n_batches = 0\n",
        "        model.train()\n",
        "        \n",
        "        for batch in train_batches:         \n",
        "            scores = model(batch.text)\n",
        "            loss = loss_function(scores, batch.label)\n",
        "            optimizer.zero_grad()            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "            n_batches += 1\n",
        "        \n",
        "        train_loss = loss_sum / n_batches\n",
        "        history['train_loss'].append(train_loss)\n",
        "        \n",
        "        n_correct = 0\n",
        "        n_valid = len(valid)\n",
        "        loss_sum = 0\n",
        "        n_batches = 0\n",
        "\n",
        "        model.eval()\n",
        "# Start evaluation\n",
        "        for batch in valid_batches:\n",
        "            scores = model(batch.text)\n",
        "            n_corr_batch, loss_batch = evaluate_validation(scores, loss_function, batch.label)\n",
        "            loss_sum += loss_batch\n",
        "            n_correct += n_corr_batch\n",
        "            n_batches += 1\n",
        "        val_acc = n_correct / n_valid\n",
        "        val_loss = loss_sum / n_batches\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if val_acc > max_val_acc:\n",
        "            max_val_acc = val_acc\n",
        "            models[lemma] = copy.deepcopy(model)\n",
        "            max_verifications[lemma] = max_val_acc\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        t1 = time.time()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch {i+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}, val acc: {val_acc:.4f}, time = {t1-t0:.4f}, lr = {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "    plt.plot(history['train_loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.plot(history['val_acc'])\n",
        "    plt.legend(['training loss', 'validation loss', 'validation accuracy'])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PFvRwlb7uAcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Bidirectional LSTM"
      ],
      "metadata": {
        "id": "8Pnd5TppP-Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_test_data(corpus_file, doc_start):\n",
        "    TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=lambda x: x.split())\n",
        "    WORDTYPE = torchtext.legacy.data.Field()\n",
        "    datafields = [('text', TEXT), ('wordtype', WORDTYPE)]\n",
        "    pad_string = '<pad>'\n",
        "    sentence_length = 160\n",
        "    half_sentence_length = int(sentence_length/2)\n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        examples = []\n",
        "        for line in f:\n",
        "            columns = line.strip().split(maxsplit=doc_start)\n",
        "            position_of_wordtype = int(columns[2])\n",
        "            doc = columns[-1]\n",
        "            doc_string_vector = doc.split()\n",
        "            temp_pad = [pad_string for x in range(0,80)]\n",
        "            swap_padding = []\n",
        "            swap_padding.extend(temp_pad)\n",
        "            swap_padding.extend(doc_string_vector)\n",
        "            swap_padding.extend(temp_pad)\n",
        "\n",
        "            sliced_doc = swap_padding[position_of_wordtype:position_of_wordtype + 160]\n",
        "            if len(sliced_doc) != 160:\n",
        "                print(sliced_doc)\n",
        "                raise RuntimeError\n",
        "\n",
        "            sliced_doc = \" \".join(sliced_doc)\n",
        "\n",
        "            wordtype = columns[1].split('.')[0]\n",
        "            examples.append(torchtext.legacy.data.Example.fromlist([sliced_doc, wordtype], datafields))\n",
        "    dataset = torchtext.legacy.data.Dataset(examples, datafields)\n",
        "    return (dataset, TEXT, WORDTYPE)\n"
      ],
      "metadata": {
        "id": "9gHsSHshximj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = read_test_data('wsd_test_blind.txt', doc_start=4)\n",
        "for example in test_dataset[0].examples:\n",
        "    test_example_string = example.text\n",
        "    example_wordtype = example.wordtype[0]\n",
        "\n",
        "    \n",
        "    example_vocab = model_vocabs[example_wordtype]\n",
        "    example_label_vocabs = model_label_vocabs[example_wordtype]\n",
        "\n",
        "    encoded_example = torch.tensor([example_vocab.stoi[x] for x in test_example_string],device=device, requires_grad = False)\n",
        "\n",
        "    model = models[example_wordtype]\n",
        "    model.lstm.flatten_parameters()\n",
        "\n",
        "    scores = model(encoded_example.unsqueeze(1))\n",
        "    prediction = scores.argmax(dim=1)\n",
        "    \n",
        "    decoded_prediction = example_label_vocabs.itos[prediction]\n",
        "    print(decoded_prediction)"
      ],
      "metadata": {
        "id": "570VYB_sxu2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report: \n",
        "\n",
        "\n",
        "In this assigment, we have implemented three different models. We applied simple neural netwrok using continuous bag-of-words approach, Unidirectional LSTM and Bidirectional LSTM.\n",
        "\n",
        "The CBoW, LSTM and BiLSTM models performed well in terms of loss and accuracy and we achived more then base model accuracy. \n",
        "\n",
        "We used the following external Python libraries:\n",
        "PyTorch to implement the neural network and some of the data management\n",
        "scikit-learn for a couple of simple utilities (LabelEncoder and train_test_split).\n",
        "matplotlib for plotting\n",
        "\n",
        "To run the program\n",
        ", these libraries need to be installed separately using pip or conda\n",
        "\n",
        "\n",
        "It is worth mentioning that we have used some functions provided during the lectures, we also got some funtions other resurces from google such as github, in order to tokanize the dataset, implement the padding and to create vocabulary.\n",
        "\n",
        "..\n",
        ".\n"
      ],
      "metadata": {
        "id": "AEYx_Nw6QN5R"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assigment1solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}